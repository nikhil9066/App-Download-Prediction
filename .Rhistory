colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
# Convert target variable V8 to a factor (binary classification)
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Calculate the correlation matrix
cor_matrix <- cor(data[, c("V1", "V2", "V3", "V4", "V5", "V6", "V7")])
# Pairwise scatterplot for numerical features
library(GGally)
# Select relevant features
feature_columns <- data[, c("V1", "V2", "V3", "V4", "V5", "V6", "V7")]
# Create a pair plot
ggpairs(feature_columns, aes(colour = data$V8)) +
labs(title = "Pairwise Scatterplot Matrix")
# Boxplot to show distribution of features by the target variable (V8)
boxplot_features <- function(feature) {
ggplot(data, aes_string(x = "V8", y = feature)) +
geom_boxplot(fill = "lightblue", color = "black") +
labs(title = paste("Boxplot of", feature, "by Download Status"), x = "App Download (V8)", y = feature) +
theme_minimal()
}
# Create boxplots for each feature
for (feature in features) {
print(boxplot_features(feature))
}
# Visualizing the distribution of numerical features for V8 (target variable)
library(ggplot2)
# Function to plot distribution for each feature
plot_feature_distribution <- function(feature, title) {
ggplot(data, aes_string(x = feature, fill = "V8")) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = title, x = feature, y = "Count") +
theme_minimal()
}
# List of features to plot
features <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7")
# Create distribution plots for each feature
for (feature in features) {
plot_feature_distribution(feature, paste("Distribution of", feature, "by Download Status"))
}
plot_feature_distribution(feature, paste("Distribution of", feature, "by Download Status"))
# Exclude V6 from the feature columns for the pair plot
feature_columns <- data[, c("V1", "V2", "V3", "V4", "V5", "V7")]
# Create the pair plot without V6
library(GGally)
ggpairs(feature_columns, aes(colour = data$V8)) +
labs(title = "Pairwise Scatterplot Matrix (Excluding V6)")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
# Load the dataset
data <- read.csv("data.csv", sep=",", header=FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
# Density plot for features by target variable (V8)
density_plot_features <- function(feature) {
ggplot(data, aes_string(x = feature, fill = "V8")) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", feature, "by Download Status"), x = feature, y = "Density") +
theme_minimal()
}
# Create density plots for each feature
for (feature in features) {
print(density_plot_features(feature))
}
data$V8 <- as.factor(data$V8)
# Density plot for features by target variable (V8)
density_plot_features <- function(feature) {
ggplot(data, aes_string(x = feature, fill = "V8")) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", feature, "by Download Status"), x = feature, y = "Density") +
theme_minimal()
}
# Create density plots for each feature
for (feature in features) {
print(density_plot_features(feature))
}
# Pairwise scatterplot for numerical features
library(GGally)
# Select relevant features
feature_columns <- data[, c("V1", "V2", "V3", "V4", "V5", "V6", "V7")]
# Create a pair plot
ggpairs(feature_columns, aes(colour = data$V8)) +
labs(title = "Pairwise Scatterplot Matrix")
# Boxplot to show distribution of features by the target variable (V8)
boxplot_features <- function(feature) {
ggplot(data, aes_string(x = "V8", y = feature)) +
geom_boxplot(fill = "lightblue", color = "black") +
labs(title = paste("Boxplot of", feature, "by Download Status"), x = "App Download (V8)", y = feature) +
theme_minimal()
}
# Create boxplots for each feature
for (feature in features) {
print(boxplot_features(feature))
}
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
# Load the dataset
data <- read.csv("data.csv", sep=",", header=FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
# Convert target variable V8 to a factor (binary classification)
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Pre-forest: Visualize distribution of target variable
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
# Pre-forest: Visualize distribution of a feature (V2)
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
# Pre-forest: Visualize distribution of target variable
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
# Load the dataset
data <- read.csv("data.csv", sep=",", header=FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
# Convert target variable V8 to a factor (binary classification)
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Pre-forest: Visualize distribution of target variable
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
# Pre-forest: Visualize distribution of a feature (V2)
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
# Pre-forest: Visualize distribution of target variable
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
# Load the dataset
data <- read.csv("data.csv", sep=",", header=FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
# Convert target variable V8 to a factor (binary classification)
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Pre-forest: Visualize distribution of target variable
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
# Pre-forest: Visualize distribution of a feature (V2)
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
# Build the initial random forest model
rf_model <- randomForest(V8 ~ V1 + V2 + V3 + V4 + V5, data = trainData)
print(rf_model)
# Post-forest: Feature importance
varImpPlot(rf_model, main = "Variable Importance")
# Predictions on test set and confusion matrix
predictions <- predict(rf_model, testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
# Post-forest: ROC curve and AUC
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
# Plotting Confusion Matrix
conf_matrix_plot <- as.data.frame(conf_matrix$table)
ggplot(conf_matrix_plot, aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 1) +
scale_fill_gradient(low = "white", high = "blue") +
labs(title = "Confusion Matrix: Random Forest", x = "Predicted", y = "Actual")
# Feature importance plot
feature_importance_plot <- varImpPlot(rf_model, main = "Variable Importance")
# Feature Engineering: Creating interaction terms
data$V2_V3_interaction <- data$V2 * data$V3
data$V4_V5_interaction <- data$V4 * data$V5
# Convert V6 and V7 (timestamps) to numeric
data$V6 <- as.numeric(as.POSIXct(data$V6, format = "%m/%d/%y %H:%M", tz = "UTC"))
data$V7[data$V7 == ""] <- NA
data$V7 <- as.numeric(as.POSIXct(data$V7, format = "%m/%d/%y %H:%M", tz = "UTC"))
# Split data after feature engineering
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Hyperparameter tuning using caret
tuneGrid <- expand.grid(mtry = 2:5)
control <- trainControl(method = "cv", number = 5)
# Train the model with tuning
rf_tuned <- train(V8 ~ V1 + V2 + V3 + V4 + V5 + V2_V3_interaction + V4_V5_interaction,
data = trainData,
method = "rf",
tuneGrid = tuneGrid,
trControl = control)
# Print tuned model results
print(rf_tuned)
# Feature importance after tuning
varImpPlot(rf_tuned$finalModel, main = "Variable Importance after Tuning")
# Predictions on test data with the tuned model
predictions <- predict(rf_tuned, newdata = testData)
# Confusion matrix and performance metrics
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
# ROC curve and AUC after tuning
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve after Tuning")
auc_value <- auc(roc_curve)
cat("AUC after tuning:", auc_value, "\n")
# Apply SMOTE for class imbalance handling
library(DMwR)
# Add interaction terms to the balanced data
trainData_balanced$V2_V3_interaction <- trainData_balanced$V2 * trainData_balanced$V3
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
# Load the dataset
data <- read.csv("data.csv", sep = ",", header = FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
# Convert target variable V8 to a factor (binary classification)
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Pre-forest: Visualize distribution of target variable
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
# Pre-forest: Visualize distribution of a feature (V2)
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
# Build the initial random forest model
rf_model <- randomForest(V8 ~ V1 + V2 + V3 + V4 + V5, data = trainData)
print(rf_model)
# Post-forest: Feature importance
varImpPlot(rf_model, main = "Variable Importance")
# Predictions on test set and confusion matrix
predictions <- predict(rf_model, testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
# Post-forest: ROC curve and AUC
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
# Plotting Confusion Matrix
conf_matrix_plot <- as.data.frame(conf_matrix$table)
ggplot(conf_matrix_plot, aes(Prediction, Reference, fill = Freq)) +
geom_tile() +
geom_text(aes(label = Freq), vjust = 1) +
scale_fill_gradient(low = "white", high = "blue") +
labs(title = "Confusion Matrix: Random Forest", x = "Predicted", y = "Actual")
# Feature importance plot
feature_importance_plot <- varImpPlot(rf_model, main = "Variable Importance")
# Feature Engineering: Creating interaction terms
data$V2_V3_interaction <- data$V2 * data$V3
data$V4_V5_interaction <- data$V4 * data$V5
# Convert V6 and V7 (timestamps) to numeric
data$V6 <- as.numeric(as.POSIXct(data$V6, format = "%m/%d/%y %H:%M", tz = "UTC"))
data$V7[data$V7 == ""] <- NA
data$V7 <- as.numeric(as.POSIXct(data$V7, format = "%m/%d/%y %H:%M", tz = "UTC"))
# Split data after feature engineering
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Hyperparameter tuning using caret
tuneGrid <- expand.grid(mtry = 2:5)
control <- trainControl(method = "cv", number = 5)
# Train the model with tuning
rf_tuned <- train(V8 ~ V1 + V2 + V3 + V4 + V5 + V2_V3_interaction + V4_V5_interaction,
data = trainData,
method = "rf",
tuneGrid = tuneGrid,
trControl = control)
# Print tuned model results
print(rf_tuned)
# Feature importance after tuning
varImpPlot(rf_tuned$finalModel, main = "Variable Importance after Tuning")
# Predictions on test data with the tuned model
predictions <- predict(rf_tuned, newdata = testData)
# Confusion matrix and performance metrics
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
# ROC curve and AUC after tuning
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve after Tuning")
auc_value <- auc(roc_curve)
cat("AUC after tuning:", auc_value, "\n")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
data <- read.csv("data.csv", sep = ",", header = FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
data <- read.csv("data.csv", sep = ",", header = FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Build the initial random forest model
rf_model <- randomForest(V8 ~ V1 + V2 + V3 + V4 + V5, data = trainData)
print(rf_model)
varImpPlot(rf_model, main = "Variable Importance")
# Predictions on test set and confusion matrix
predictions <- predict(rf_model, testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
data <- read.csv("data.csv", sep = ",", header = FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Build the initial random forest model
rf_model <- randomForest(V8 ~ V1 + V2 + V3 + V4 + V5, data = trainData)
print(rf_model)
varImpPlot(rf_model, main = "Variable Importance")
# Predictions on test set and confusion matrix
predictions <- predict(rf_model, testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
tuneGrid <- expand.grid(mtry = 2:5)
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(V8 ~ V1 + V2 + V3 + V4 + V5,
data = trainData,
method = "rf",
tuneGrid = tuneGrid,
trControl = control)
tuneGrid <- expand.grid(mtry = 2:5)
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(V8 ~ V1 + V2 + V3 + V4 + V5,
data = trainData,
method = "rf",
tuneGrid = tuneGrid,
trControl = control)
print(rf_tuned)
tuneGrid <- expand.grid(mtry = 2:5)
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(V8 ~ V1 + V2 + V3 + V4 + V5,
data = trainData,
method = "rf",
tuneGrid = tuneGrid,
trControl = control)
print(rf_tuned)
varImpPlot(rf_tuned$finalModel, main = "Variable Importance after Tuning")
predictions <- predict(rf_tuned, newdata = testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve after Tuning")
auc_value <- auc(roc_curve)
cat("AUC after tuning:", auc_value, "\n")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
data <- read.csv("data.csv", sep = ",", header = FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
ggplot(data, aes(x = V8)) +
geom_bar(fill = "steelblue") +
labs(title = "Distribution of App Download (V8)", x = "App Download (0 or 1)", y = "Count")
ggplot(data, aes(x = V2, fill = V8)) +
geom_histogram(binwidth = 1, color = "black", position = "dodge") +
labs(title = "Distribution of V2 (App ID) by Download Status", x = "App ID (V2)", y = "Count")
data$V8 <- as.factor(data$V8)
# Split the data into 50% training and 50% testing
trainIndex <- createDataPartition(data$V8, p = 0.5, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]
# Build the initial random forest model
rf_model <- randomForest(V8 ~ V1 + V2 + V3 + V4 + V5, data = trainData)
print(rf_model)
varImpPlot(rf_model, main = "Variable Importance")
# Predictions on test set and confusion matrix
predictions <- predict(rf_model, testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve for Random Forest Model")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
tuneGrid <- expand.grid(mtry = 2:5)
control <- trainControl(method = "cv", number = 5)
rf_tuned <- train(V8 ~ V1 + V2 + V3 + V4 + V5,
data = trainData,
method = "rf",
tuneGrid = tuneGrid,
trControl = control)
print(rf_tuned)
varImpPlot(rf_tuned$finalModel, main = "Variable Importance after Tuning")
predictions <- predict(rf_tuned, newdata = testData)
conf_matrix <- confusionMatrix(predictions, testData$V8)
print(conf_matrix)
roc_curve <- roc(testData$V8, as.numeric(predictions))
plot(roc_curve, col = "blue", main = "ROC Curve after Tuning")
auc_value <- auc(roc_curve)
cat("AUC after tuning:", auc_value, "\n")
library(tidyverse)
library(randomForest)
library(ggplot2)
library(pROC)
library(caret)
library(gridExtra)
library(reshape2)
library(RColorBrewer)
set.seed(555)
data <- read.csv("data.csv", sep = ",", header = FALSE)
colnames(data) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8")
